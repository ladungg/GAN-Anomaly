# üìä ƒê√°nh Gi√° M√¥ H√¨nh GAN Anomaly Detection

## üéØ T√≥m T·∫Øt Chung

M√¥ h√¨nh **GANAnomaly** l√† m·ªôt **GAN-based Anomaly Detection** system ƒë∆∞·ª£c x√¢y d·ª±ng t·ªët ƒë·ªÉ ph√°t hi·ªán c√°c cu·ªôc t·∫•n c√¥ng m·∫°ng. K·∫øt qu·∫£ hi·ªán t·∫°i cho th·∫•y m√¥ h√¨nh ho·∫°t ƒë·ªông **kh√° t·ªët** v·ªõi ROC-AUC ƒë·∫°t **0.983**.

---

## ‚úÖ ƒêi·ªÉm M·∫°nh

### 1. **Ki·∫øn Tr√∫c M·∫°nh M·∫Ω**
- **Generator (NetG)**: Encoder-Decoder architecture v·ªõi bottleneck ƒë·ªÉ n√©n th√¥ng tin
- **Discriminator (NetD)**: Ph√¢n lo·∫°i ƒë·∫ßu ra v√† tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng
- **S·ª≠ d·ª•ng 1D CNN**: Ph√π h·ª£p ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu m·∫°ng (NetFlow)
- **Bottleneck Architecture**: T√¥i h·ªçc bi·ªÉu di·ªÖn latent t·ªët cho anomaly detection

### 2. **Hi·ªáu Su·∫•t T·ªët**
```
ROC-AUC (Best): 0.983
```
- ƒê√¢y l√† m·ªôt k·∫øt qu·∫£ **r·∫•t t·ªët** (>0.95) cho anomaly detection
- M√¥ h√¨nh h·ªçc ·ªïn ƒë·ªãnh qua c√°c epoch (kh√¥ng overfitting qu√° m·ª©c)

### 3. **Loss Functions Ph√π H·ª£p**
- **Adversarial Loss (L2)**: `w_adv = 1.0` - B·∫Øt bu·ªôc generator t·∫°o d·ªØ li·ªáu gi·ªëng th·ª±c
- **Reconstruction Loss (L1)**: `w_con = 50` - ƒê·∫£m b·∫£o generator h·ªçc t√°i t·∫°o d·ªØ li·ªáu
- **Encoding Loss (L2)**: `w_enc = 1.0` - Gi·ªØ cho latent space consistent

### 4. **C·∫•u H√¨nh Hu·∫•n Luy·ªán T·ªët**
- **Learning Rate**: 0.0002 - Ph√π h·ª£p ƒë·ªÉ GAN h·ªôi t·ª• ·ªïn ƒë·ªãnh
- **Batch Size**: 64 - ƒê·ªß l·ªõn cho gradient ·ªïn ƒë·ªãnh
- **Optimizer**: Adam v·ªõi Œ≤1=0.5 - Chu·∫©n cho GAN training

### 5. **Metrics Ho√†n Ch·ªânh**
- ƒêo l∆∞·ªùng ROC-AUC
- L∆∞u anomaly scores
- T√≠nh Confusion Matrix
- Inference time tracking (‚âà13ms/batch)

---

## ‚ö†Ô∏è C√°c V·∫•n ƒê·ªÅ & C·∫£nh B√°o

### 1. **Hi·ªáu Su·∫•t B·∫•t ·ªîn**
```
ROC Scores: 0.917 ‚Üí 0.983 ‚Üí 0.701 ‚Üí 0.962 ‚Üí ...
```
- **V·∫•n ƒë·ªÅ**: ROC score r∆°i xu·ªëng 0.701 sau ƒë√≥ 0.786, 0.827, 0.871
- **Nguy√™n nh√¢n**: C√≥ th·ªÉ do:
  - Discriminator b·ªã collapse (loss < 1e-5 triggering reinit_d)
  - M√¥ h√¨nh ch∆∞a h·ªôi t·ª• ho√†n to√†n

### 2. **Confusion Matrix Cho Th·∫•y M·∫•t C√¢n B·∫±ng**
```
Predicted:      Normal    Anomaly
Actually Normal: 47,892      2,108  (4.2% False Positive)
Actually Anomaly:    83        417  (16.6% False Negative)
```
- **TPR (Recall)**: 83.4% (417/(417+83))
- **TNR (Specificity)**: 95.8% (47,892/(47,892+2,108))
- **FPR**: 4.2% - Kh√° cao, c√≥ th·ªÉ g√¢y r·ªëi lo·∫°n

### 3. **Thi·∫øu Early Stopping**
```python
# Ch·ªâ c√≥:
if res[self.opt.metric] > best_auc:
    best_auc = res[self.opt.metric]
    self.save_weights(self.epoch)
```
- M√¥ h√¨nh kh√¥ng d·ª´ng khi ROC kh√¥ng c·∫£i thi·ªán
- Ch·ªâ hu·∫•n luy·ªán 5 epoch (qu√° √≠t!)

### 4. **Threshold C·ª©ng**
```python
threshold = np.percentile(prob, 95)  # 95th percentile
```
- Ng∆∞·ª°ng ph√°t hi·ªán d·ª±a tr√™n percentile l√† **arbitrary**
- Kh√¥ng t·ªëi ∆∞u h√≥a cho m·ª•c ti√™u c·ª• th·ªÉ (precision vs recall tradeoff)

### 5. **Generator Collapse Risk**
```python
if self.err_d.item() < 1e-5: self.reinit_d()
```
- Khi D loss qu√° nh·ªè ‚Üí reinit D
- ƒêi·ªÅu n√†y c√≥ th·ªÉ g√¢y m·∫•t ·ªïn ƒë·ªãnh trong training

---

## üìà Khuy·∫øn Ngh·ªã C·∫£i Thi·ªán

### 1Ô∏è‚É£ **Th√™m Early Stopping**
```python
patience = 5  # D·ª´ng n·∫øu ROC kh√¥ng c·∫£i thi·ªán 5 epoch
if res[self.opt.metric] > best_auc:
    best_auc = res[self.opt.metric]
    self.save_weights(self.epoch)
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= patience:
        print(f"Early stopping at epoch {self.epoch}")
        break
```

### 2Ô∏è‚É£ **TƒÉng S·ªë Epoch & Gi·∫£m Learning Rate ƒê·ªông**
```json
{
  "niter": 50,  // TƒÉng t·ª´ 5 l√™n 50
  "lr_schedule": {
    "enabled": true,
    "step_size": 10,
    "gamma": 0.9
  }
}
```

### 3Ô∏è‚É£ **T·ªëi ∆Øu Threshold B·∫±ng ROC Curve**
```python
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(gt_labels_np, prob)
# Ch·ªçn threshold maximize F1-score ho·∫∑c m·ª•c ti√™u c·ª• th·ªÉ
```

### 4Ô∏è‚É£ **C·∫£i Thi·ªán Discriminator**
```python
# Th·ª≠ Spectral Normalization ho·∫∑c Gradient Penalty
from torch.nn.utils.spectral_norm import spectral_norm
self.classifier = nn.Sequential(spectral_norm(layers[-1]))
```

### 5Ô∏è‚É£ **TƒÉng D·ªØ Li·ªáu ƒê√£ Hu·∫•n Luy·ªán**
```python
"proportion": 0.1,  # Hi·ªán t·∫°i ch·ªâ d√πng 10% d·ªØ li·ªáu
# Th·ª≠: 0.3, 0.5, 1.0
```

### 6Ô∏è‚É£ **Monitoring & Visualization**
```python
# L∆∞u loss curves, ROC curves cho m·ªói epoch
plot_loss_new(...)  # ƒê√£ c√≥, nh∆∞ng h√£y th√™m early stopping visualization
plot_ROC(...)       # T·ªët, ti·∫øp t·ª•c
```

---

## üìä K·∫øt Qu·∫£ Hi·ªán T·∫°i

| Metric | Gi√° Tr·ªã | ƒê√°nh Gi√° |
|--------|---------|---------|
| **ROC-AUC** | 0.983 | ‚úÖ T·ªët |
| **Inference Time** | ~13ms/batch | ‚úÖ Nhanh |
| **TPR (Recall)** | 83.4% | ‚ö†Ô∏è C√≥ th·ªÉ t·ªët h∆°n |
| **TNR (Specificity)** | 95.8% | ‚úÖ T·ªët |
| **FPR** | 4.2% | ‚ö†Ô∏è C√≥ th·ªÉ gi·∫£m |

---

## üéì K·∫øt Lu·∫≠n

‚úÖ **M√¥ h√¨nh c·ªßa b·∫°n T·ªêT** - ROC-AUC 0.983 l√† k·∫øt qu·∫£ r·∫•t t·ªët cho anomaly detection.

Tuy nhi√™n, c√≥ nh·ªØng c·∫£i thi·ªán c·∫ßn thi·∫øt:
1. **·ªîn ƒë·ªãnh ROC score** - Hi·ªán t·∫°i c√≥ bi·∫øn ƒë·ªông l·ªõn
2. **Gi·∫£m False Positive** - Gi·∫£m t·ª´ 4.2% xu·ªëng 1-2%
3. **TƒÉng ƒë·ªô b·ªÅn** - Th√™m early stopping, regularization
4. **T·ªëi ∆∞u threshold** - Kh√¥ng d√πng percentile c·ª©ng nh·∫Øc

Sau khi c·∫£i thi·ªán, b·∫°n c√≥ th·ªÉ ƒë·∫°t **ROC-AUC > 0.99** v·ªõi **FPR < 1%**.

---

## üìù File Li√™n Quan
- Training: [train.py](GANAnomaly/train.py)
- Model: [lib/model.py](GANAnomaly/lib/model.py)
- Networks: [lib/networks.py](GANAnomaly/lib/networks.py)
- Config: [config.json](GANAnomaly/config.json)
